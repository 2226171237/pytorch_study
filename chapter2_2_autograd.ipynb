{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTqN0ZCkFVPL"
   },
   "source": [
    "# autograd\n",
    "用Tensor训练网络很方便，但从上一小节最后的线性回归例子来看，反向传播过程需要手动实现。这对于像线性回归等较为简单的模型来说，还可以应付，但实际使用中经常出现非常复杂的网络结构，此时如果手动实现反向传播，不仅费时费力，而且容易出错，难以检查。torch.autograd就是为方便用户使用，而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。\n",
    "\n",
    "计算图(Computation Graph)是现代深度学习框架如PyTorch和TensorFlow等的核心，其为高效自动求导算法——反向传播(Back Propogation)提供了理论支持，了解计算图在实际写程序过程中会有极大的帮助。本节将涉及一些基础的计算图知识，但并不要求读者事先对此有深入的了解。关于计算图的基础知识推荐阅读Christopher Olah的文章.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hWhlNRVFrJ4"
   },
   "source": [
    "##  requires_grad\n",
    "\n",
    "PyTorch在autograd模块中实现了计算图的相关功能，autograd中的核心数据结构是Variable。从v0.4版本起，Variable和Tensor合并。我们可以认为需要求导(requires_grad)的tensor即Variable. autograd记录对tensor的操作记录用来构建计算图。\n",
    "\n",
    "Variable提供了大部分tensor支持的函数，但其不支持部分`inplace`函数，因这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算反向传播梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的`backward`方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。\n",
    "\n",
    "`variable.backward(gradient=None, retain_graph=None, create_graph=None)`主要有如下参数：\n",
    "\n",
    "- grad_variables：形状与variable一致，对于`y.backward()`，grad_variables相当于链式法则${dz \\over dx}={dz \\over dy} \\times {dy \\over dx}$中的$\\textbf {dz} \\over \\textbf {dy}$。grad_variables也可以是tensor或序列。\n",
    "- retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。\n",
    "- create_graph：对反向传播过程再次构建计算图，可通过`backward of backward`实现求高阶导数。\n",
    "\n",
    "上述描述可能比较抽象，如果没有看懂，不用着急，会在本节后半部分详细介绍，下面先看几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I8nK4ISGB7L"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "iAEbWZy4GQjv",
    "outputId": "340a96c3-13b1-4f98-be99-c46d239c7f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8873, -0.4227, -1.8543, -0.6434],\n",
       "        [-0.2812,  1.3114, -0.0459, -0.2372],\n",
       "        [ 1.2774,  0.8956, -0.3374, -0.5551]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意在创建Tensor的时候需要指定requires_grad\n",
    "a=t.randn(3,4,requires_grad=True)\n",
    "\n",
    "#或者\n",
    "a=t.randn(3,4).requires_grad_()\n",
    "\n",
    "#或者\n",
    "a=t.randn(3,4)\n",
    "a.requires_grad=True\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Fek2da5IHURo",
    "outputId": "518b239c-844e-493b-ceab-af7bf4ff6376"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=t.zeros(3,4).requires_grad_()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "rnFtPlF-Hk1U",
    "outputId": "386beef2-70f7-4de4-e898-7b4a71e17b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8873, -0.4227, -1.8543, -0.6434],\n",
       "        [-0.2812,  1.3114, -0.0459, -0.2372],\n",
       "        [ 1.2774,  0.8956, -0.3374, -0.5551]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=a+b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRAPURBPHqNp"
   },
   "outputs": [],
   "source": [
    "d=c.sum()\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5zzt2F9nHu5W",
    "outputId": "9ca18e84-14e8-4b41-87a2-ca08fd216524"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d # d还是一个requires_grad=True的tensor,对它的操作需要慎重\n",
    "d.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "jAmsmBTqH2zC",
    "outputId": "a10cd73a-98de-4e43-83a0-a8a434dda094"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "17NCQo5bH4__",
    "outputId": "101748f6-4b3d-4340-e024-a45d2449a021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导，\n",
    "# 因此c的requires_grad属性会自动设为True\n",
    "a.requires_grad,b.requires_grad,c.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RUGGqFIxIGkx",
    "outputId": "2fc734a0-2251-4f91-bd0f-2564e216b8df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由用户创建的variable属于叶子节点，对应的grad_fn是None,\n",
    "a.is_leaf,b.is_leaf,c.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hLQNbxRYISyY",
    "outputId": "90b950f4-d004-4527-f914-f4d890e450a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x2ad8f658cf8>, True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c.grad是None, 因c不是叶子节点，它的梯度是用来计算a的梯度\n",
    "# 所以虽然c.requires_grad = True,但其梯度计算完之后即被释放\n",
    "c.grad_fn,c.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a38C1GozIeqV"
   },
   "source": [
    "计算下面这个函数的导函数：\n",
    "$$\n",
    "y = x^2\\bullet e^x\n",
    "$$\n",
    "它的导函数是：\n",
    "$$\n",
    "{dy \\over dx} = 2x\\bullet e^x + x^2 \\bullet e^x\n",
    "$$\n",
    "来看看autograd的计算结果与手动求导计算结果的误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3b7er_iImk8"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  y=x**2*t.exp(x)\n",
    "  return y\n",
    "\n",
    "def gradf(x):\n",
    "  dx=2*x*t.exp(x)+x**2*t.exp(x)\n",
    "  return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "OuSyZE2tI7tk",
    "outputId": "d6a370ba-35da-4dbb-fe43-4b488abe8996"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3338e-01, 3.8330e+00, 1.0013e+01, 4.1377e-01],\n",
       "        [1.8173e-01, 4.4784e-01, 9.8027e+00, 3.1920e-01],\n",
       "        [3.4684e-01, 6.1543e-02, 3.2282e-01, 6.4124e+01]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.randn(3,4,requires_grad=True)\n",
    "y=f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "r5OiKQ0yJEPF",
    "outputId": "067c4467-83eb-40c8-c9c9-fd7799572ae7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.2896,  10.6843,  23.3904,  -0.3159],\n",
       "        [  1.2008,  -0.2683,  22.9792,   1.7349],\n",
       "        [ -0.3877,  -0.3684,  -0.4076, 117.4412]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(t.ones(y.size())) #gradient形状与y一致\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "kQyQFzWIJZVV",
    "outputId": "a3237db8-c644-4eff-c1d6-946fdabeba8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.2896,  10.6843,  23.3904,  -0.3159],\n",
       "        [  1.2008,  -0.2683,  22.9792,   1.7349],\n",
       "        [ -0.3877,  -0.3684,  -0.4076, 117.4412]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiUyTSp4J5dw"
   },
   "source": [
    "## 计算图\n",
    "PyTorch中`autograd`的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$ \\textbf {z = wx + b}$可分解为$\\textbf{y = wx}$和$\\textbf{z = y + b}$，其计算图如图3-3所示，图中`MUL`，`ADD`都是算子，$\\textbf{w}$，$\\textbf{x}$，$\\textbf{b}$即变量。\n",
    "\n",
    "![图3-3:computation graph](http://localhost:8888/notebooks/pytorch/pytorch-book-master/chapter3-Tensor%E5%92%8Cautograd/imgs/com_graph.svg)\n",
    "\n",
    "如上有向无环图中，$\\textbf{X}$和$\\textbf{b}$是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。$\\textbf{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。\n",
    "$${\\partial z \\over \\partial b} = 1,\\space {\\partial z \\over \\partial y} = 1\\\\\n",
    "{\\partial y \\over \\partial w }= x,{\\partial y \\over \\partial x}= w\\\\\n",
    "{\\partial z \\over \\partial x}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial x}=1 * w\\\\\n",
    "{\\partial z \\over \\partial w}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial w}=1 * x\\\\\n",
    "$$\n",
    "而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，其过程如图3-4所示。\n",
    "\n",
    "![图3-4：计算图的反向传播](http://localhost:8888/notebooks/pytorch/pytorch-book-master/chapter3-Tensor%E5%92%8Cautograd/imgs/com_graph_backward.svg)\n",
    "\n",
    "\n",
    "在PyTorch实现中，autograd会随着用户的操作，记录生成当前variable的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作`Function`，每一个变量在图中的位置可通过其`grad_fn`属性在图中的位置推测得到。在反向传播过程中，autograd沿着这个图从当前变量（根节点$\\textbf{z}$）溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以`Backward`结尾。下面结合代码学习autograd的实现细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_Lm2iW1KQ0Y"
   },
   "outputs": [],
   "source": [
    "x=t.ones(1)\n",
    "b=t.rand(1,requires_grad=True)\n",
    "w=t.rand(1,requires_grad=True)\n",
    "y=w*x\n",
    "z=y+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W35WFzt5LTqy",
    "outputId": "fc27354b-0d65-493e-9ed0-b9cb579bee44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad,b.requires_grad,w.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JaWpn142LZ1c",
    "outputId": "74a3a51c-cd24-4725-ff00-3c0ebeb79d22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w\n",
    "# 故而y.requires_grad为True\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YL6tJLS1LlXW",
    "outputId": "b888d55f-9b0d-42ba-885a-4640bf495a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf,w.is_leaf,b.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v_sdSMfmLsXA",
    "outputId": "c0d293bb-477e-4a4c-cada-dc72695bcf91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf,z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YWeEcSghLvvG",
    "outputId": "e0a13389-99c9-4a22-fd22-c27cf7d1f5ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x2ad8f658e10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_fn可以查看这个variable的反向传播函数，\n",
    "# z是add函数的输出，所以它的反向传播函数是AddBackward\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "KAft95u3Lxvc",
    "outputId": "5275532a-eb58-4ed0-bfff-8d982bd59f0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x2ad8f659278>, 0), (<AccumulateGrad at 0x2ad8f659198>, 0))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next_functions保存grad_fn的输入，是一个tuple，tuple的元素也是Function\n",
    "# 第一个是y，它是乘法(mul)的输出，所以对应的反向传播函数y.grad_fn是MulBackward\n",
    "# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有\n",
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1oxb_caOMClC",
    "outputId": "bca67b22-16e7-47dc-e67c-cb95f6c513f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable的grad_fn对应着和图中的function相对应\n",
    "z.grad_fn.next_functions[0][0]==y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zYsN8eNtMrsG",
    "outputId": "ddef2716-d6ea-4f04-e07d-5cc0a07dbeb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x2ad8f659470>, 0), (None, 0))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个是w，叶子节点，需要求导，梯度是累加的\n",
    "# 第二个是x，叶子节点，不需要求导，所以为None\n",
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pA5hqv9sM1W3",
    "outputId": "dc9ef47f-1b8e-4706-c833-f96265010929"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 叶子节点的grad_fn是None\n",
    "w.grad_fn,x.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-U_ZyDNM-DB"
   },
   "source": [
    "计算w的梯度的时候，需要用到x的数值(${\\partial y\\over \\partial w} = x $)，这些中间数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定`retain_graph`来保留这些buffer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "35zoNcAbNK5n",
    "outputId": "5b665e29-ff46-4cc9-caa5-fb951713818e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用retain_graph来保存buffer\n",
    "z.backward(retain_graph=True)\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iwbjIi0mNcMz",
    "outputId": "fd7df28a-5001-48c5-c4f2-9fdff43e0954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义\n",
    "z.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.arange(0,3,dtype=t.float32,requires_grad=True)\n",
    "y=x**2+x*2\n",
    "z=y.sum()\n",
    "z.backward()## 从z开始反向传播 ,z是个标量\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x**2+x*2\n",
    "z=y.sum()\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fv6X_EgZNlmL"
   },
   "source": [
    "PyTorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建，所以它能够使用Python控制语句（如for、if等）根据需求创建计算图。这点在自然语言处理领域中很有用，它意味着你不需要事先构建所有可能用到的图的路径，图在运行时才构建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uP0aKMUYNqJi",
    "outputId": "833465a1-e08b-4493-c7f4-5b37af138df6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def abs(x):\n",
    "  if x.data[0]>0:\n",
    "    return x\n",
    "  else: \n",
    "    return -x\n",
    " \n",
    "x=t.ones(1,requires_grad=True)\n",
    "y=abs(x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b7RxfLs2OrKa",
    "outputId": "7d34f9de-a6b0-43c6-ca05-005402f7f891"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=-1*t.ones(1)\n",
    "x.requires_grad=True\n",
    "y=abs(x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "M2HA_8GmOuxo",
    "outputId": "9bb51ea4-6b64-4976-8557-11b11b59d0b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 6., 3., 2.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    result=1\n",
    "    for ii in x:\n",
    "        if ii.item()>0:\n",
    "            result=ii*result\n",
    "    return result\n",
    "  \n",
    "x=t.arange(-2,4,dtype=t.float32,requires_grad=True)\n",
    "y=f(x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k5YS2mnTPQnN",
    "outputId": "3193d3ab-5e51-4b65-a00a-7018bfc4f4db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6doHd4-pPZlm"
   },
   "source": [
    "变量的`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都是True。这其实很好理解，对于$ \\textbf{x}\\to \\textbf{y} \\to \\textbf{z}$，x.requires_grad = True，当需要计算$\\partial z \\over \\partial x$时，根据链式法则，$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$，自然也需要求$ \\frac{\\partial z}{\\partial y}$，所以y.requires_grad会被自动标为True. \n",
    "\n",
    "\n",
    "\n",
    "有些时候我们可能不希望autograd对tensor求导。认为求导需要缓存许多中间结构，增加额外的内存/显存开销，那么我们可以关闭自动求导。对于不需要反向传播的情景（如inference，即测试推理时），关闭自动求导可实现一定程度的速度提升，并节省约一半显存，因其不需要分配空间计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.ones(1,requires_grad=True)\n",
    "w=t.rand(1,requires_grad=True)\n",
    "y=x*w\n",
    "# y依赖于w，而w.requires_grad = True\n",
    "y.requires_grad,w.requires_grad,x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with t.no_grad():\n",
    "    x=t.ones(1)\n",
    "    w=t.rand(1,requires_grad=True)\n",
    "    y=x*w\n",
    "# y依赖于w和x，虽然w.requires_grad = True，但是y的requires_grad依旧为False\n",
    "y.requires_grad,w.requires_grad,x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.no_grad??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "x=t.ones(1)\n",
    "w=t.rand(1,requires_grad=True)\n",
    "y=w*x\n",
    "# y依赖于w和x，虽然w.requires_grad = True，但是y的requires_grad依旧为False\n",
    "y.requires_grad,w.requires_grad,x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2ad8f659a20>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 恢复默认配置\n",
    "t.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要修改tensor的数值，但是又不希望被autograd记录，那么我么可以对tensor.data进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=t.ones(3,4,requires_grad=True)\n",
    "b=t.ones(3,4,requires_grad=True)\n",
    "c=a*b\n",
    "\n",
    "a.data #还是一个tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data.requires_grad #但是已经时独立于计算图之外的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=a.data.sigmoid_()\n",
    "d.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
       "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
       "        [0.7311, 0.7311, 0.7311, 0.7311]], requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们希望对tensor，但是又不希望被记录, 可以使用tensor.data 或者tensor.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 近似于 tensor=a.data, 但是如果tensor被修改，backward可能会报错\n",
    "tensor=a.detach()\n",
    "tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计tensor的一些指标，不希望被记录\n",
    "mean=tensor.mean()\n",
    "std=tensor.std()\n",
    "maximum=tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[0]=1\n",
    "# 下面会报错：　RuntimeError: one of the variables needed for gradient\n",
    "#             computation has been modified by an inplace operation\n",
    "#　因为 c=a*b, b的梯度取决于a，现在修改了tensor，其实也就是修改了a，梯度不再准确\n",
    "# c.sum().backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有两种方法：\n",
    "\n",
    "* 使用autograd.grad函数\n",
    "* 使用hook\n",
    "\n",
    "autograd.grad和hook方法都是很强大的工具，更详细的用法参考官方api文档，这里举例说明基础的使用。推荐使用hook方法，但是在实际使用中应尽量避免修改grad的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.ones(3,requires_grad=True)\n",
    "w=t.rand(3,requires_grad=True)\n",
    "y=w*x\n",
    "# y依赖于w，而w.requires_grad = True\n",
    "z=y.sum()\n",
    "x.requires_grad,w.requires_grad,y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9194, 0.9469, 0.8179]), tensor([1., 1., 1.]), None, None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非叶子节点grad计算完之后自动清空，y.grad是None\n",
    "z.backward()\n",
    "(x.grad,w.grad,y.grad,z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]),)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一种方法：使用grad获取中间变量的梯度\n",
    "x=t.ones(3,requires_grad=True)\n",
    "w=t.rand(3,requires_grad=True)\n",
    "y=w*x\n",
    "# y依赖于w，而w.requires_grad = True\n",
    "z=y.sum()\n",
    "t.autograd.grad(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y的梯度： tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 第二种方法：使用hook\n",
    "# hook是一个函数，输入是梯度，不应该有返回值\n",
    "def variable_hook(grad):\n",
    "    print('y的梯度：',grad)\n",
    "x=t.ones(3,requires_grad=True)\n",
    "w=t.rand(3,requires_grad=True)\n",
    "y=w*x\n",
    "#注册hook\n",
    "hook_handle=y.register_hook(variable_hook)\n",
    "z=y.sum()\n",
    "z.backward()\n",
    "\n",
    "# 除非你每次都要用hook，否则用完之后记得移除hook\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后再来看看variable中grad属性和backward函数`grad_variables`参数的含义，这里直接下结论：\n",
    "\n",
    "- variable $\\textbf{x}$的梯度是目标函数${f(x)} $对$\\textbf{x}$的梯度，$\\frac{df(x)}{dx} = (\\frac {df(x)}{dx_0},\\frac {df(x)}{dx_1},...,\\frac {df(x)}{dx_N})$，形状和$\\textbf{x}$一致。\n",
    "- 对于y.backward(grad_variables)中的grad_variables相当于链式求导法则中的$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$中的$\\frac{\\partial z}{\\partial y}$。z是目标函数，一般是一个标量，故而$\\frac{\\partial z}{\\partial y}$的形状与variable $\\textbf{y}$的形状一致。`z.backward()`在一定程度上等价于y.backward(grad_y)。`z.backward()`省略了grad_variables参数，是因为$z$是一个标量，而$\\frac{\\partial z}{\\partial z} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.arange(0,3,dtype=t.float32,requires_grad=True)\n",
    "y=x**2+x*2\n",
    "z=y.sum()\n",
    "z.backward()## 从z开始反向传播 ,z是个标量\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.arange(0.0,3.0,requires_grad=True)\n",
    "y=x**2+x*2\n",
    "z=y.sum()\n",
    "y_gradient=t.Tensor([1,1,1])#dz/dy\n",
    "y.backward(y_gradient) #y是个向量\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外值得注意的是，只有对variable的操作才能使用autograd，如果对variable的data直接进行操作，将无法使用反向传播。除了对参数初始化，一般我们不会修改variable.data的值。\n",
    "\n",
    "在PyTorch中计算图的特点可总结如下：\n",
    "\n",
    "* autograd根据用户对variable的操作构建其计算图。对变量的操作抽象为Function。\n",
    "* 对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。叶子节点中需要求导的variable，具有AccumulateGrad标识，因其梯度是累加的。\n",
    "* variable默认是不需要求导的，即requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。\n",
    "* variable的volatile属性默认为False，如果某一个variable的volatile属性被设为True，那么所有依赖它的节点volatile属性都为True。volatile属性为True的节点不会求导，volatile的优先级比requires_grad高。\n",
    "* 多次反向传播时，梯度是累加的。反向传播的**中间缓存**会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。\n",
    "* 非叶子节点的梯度计算完之后即被清空，可以使用autograd.grad或hook技术获取非叶子节点的值。\n",
    "* variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播\n",
    "* 反向传播函数backward的参数grad_variables可以看成链式求导的中间结果，如果是标量，可以省略，默认为1\n",
    "* PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。\n",
    "\n",
    "这些知识不懂大多数情况下也不会影响对pytorch的使用，但是掌握这些知识有助于更好的理解pytorch，并有效的避开很多陷阱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扩展autograd\n",
    "目前绝大多数函数都可以使用`autograd`实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办? 写一个`Function`，实现它的前向传播和反向传播代码，`Function`对应于计算图中的矩形， 它接收参数，计算并返回结果。下面给出一个例子。\n",
    "\n",
    "```python\n",
    "\n",
    "class Mul(Function):\n",
    "                                                            \n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b, x_requires_grad = True):\n",
    "        ctx.x_requires_grad = x_requires_grad\n",
    "        ctx.save_for_backward(w,x)\n",
    "        output = w * x + b\n",
    "        return output\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        w,x = ctx.saved_tensors\n",
    "        grad_w = grad_output * x\n",
    "        if ctx.x_requires_grad:\n",
    "            grad_x = grad_output * w\n",
    "        else:\n",
    "            grad_x = None\n",
    "        grad_b = grad_output * 1\n",
    "        return grad_w, grad_x, grad_b, None\n",
    "```\n",
    "\n",
    "分析如下：\n",
    "\n",
    "- 自定义的Function需要继承autograd.Function，没有构造函数`__init__`，forward和backward函数都是静态方法\n",
    "- backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应\n",
    "- backward函数的grad_output参数即t.autograd.backward中的`grad_variables`\n",
    "- 如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可\n",
    "- 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放\n",
    "\n",
    "Function的使用利用Function.apply(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "class MultiplyAdd(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,w,x,b):\n",
    "        ctx.save_for_backward(w,x)\n",
    "        output=w*x+b\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        w,x=ctx.saved_tensors\n",
    "        grad_w=grad_output*x\n",
    "        grad_x=grad_output*w\n",
    "        grad_b=grad_output*1\n",
    "        return grad_w,grad_x,grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([0.0143], grad_fn=<MulBackward0>), tensor([1.]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.ones(1)\n",
    "w=t.rand(1,requires_grad=True)\n",
    "b=t.rand(1,requires_grad=True)\n",
    "\n",
    "#开始前向传播\n",
    "z=MultiplyAdd.apply(w,x,b)\n",
    "\n",
    "#print('开始反向传播')\n",
    "# 调用MultiplyAdd.backward\n",
    "# 输出grad_w, grad_x, grad_b\n",
    "z.grad_fn.apply(t.ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之所以forward函数的输入是tensor，而backward函数的输入是variable，是为了实现高阶求导。backward函数的输入输出虽然是variable，但在实际使用时autograd.Function会将输入variable提取为tensor，并将计算结果的tensor封装成variable返回。在backward函数中，之所以也要对variable进行操作，是为了能够计算梯度的梯度（backward of backward）。下面举例说明，有关torch.autograd.grad的更详细使用请参照文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.], grad_fn=<MulBackward0>),)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t.tensor([5.0],requires_grad=True)\n",
    "y=x**2\n",
    "grad_x=t.autograd.grad(y,x,create_graph=True)\n",
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.]),)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_grad_x=t.autograd.grad(grad_x[0],x)\n",
    "grad_grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种设计虽然能让autograd具有高阶求导功能，但其也限制了Tensor的使用，因autograd中反向传播的函数只能利用当前已经有的Variable操作。这个设计是在0.2版本新加入的，为了更好的灵活性，也为了兼容旧版本的代码，PyTorch还提供了另外一种扩展autograd的方法。PyTorch提供了一个装饰器@once_differentiable，能够在backward函数中自动将输入的variable提取成tensor，把计算结果的tensor自动封装成variable。有了这个特性我们就能够很方便的使用numpy/scipy中的函数，操作不再局限于variable所支持的操作。但是这种做法正如名字中所暗示的那样只能求导一次，它打断了反向传播图，不再支持高阶求导。\n",
    "\n",
    "上面所描述的都是新式Function，还有个legacy Function，可以带有__init__方法，forward和backwad函数也不需要声明为@staticmethod，但随着版本更迭，此类Function将越来越少遇到，在此不做更多介绍。\n",
    "\n",
    "此外在实现了自己的Function之后，还可以使用gradcheck函数来检测实现是否正确。gradcheck通过数值逼近来计算梯度，可能具有一定的误差，通过控制eps的大小可以控制容忍的误差。 关于这部份的内容可以参考github上开发者们的讨论^3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,x):\n",
    "        output=1/(1+t.exp(-x))\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        output,=ctx.saved_tensors\n",
    "        grad_x=output*(1-output)*grad_output\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\liyajie\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\gradcheck.py:170: UserWarning: At least one of the inputs that requires gradient is not of double precision floating point. This check will likely fail if all the inputs are not of double precision floating point. \n",
      "  'At least one of the inputs that requires gradient '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 采用数值逼近方式检验计算梯度的公式对不对\n",
    "test_input=t.randn(3,4,requires_grad=True)\n",
    "t.autograd.gradcheck(Sigmoid.apply,(test_input,),eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215 µs ± 30.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "191 µs ± 2.31 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "164 µs ± 12.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def f_sigmoid(x):\n",
    "    y = Sigmoid.apply(x)\n",
    "    y.backward(t.ones(x.size()))\n",
    "    \n",
    "def f_naive(x):\n",
    "    y =  1/(1 + t.exp(-x))\n",
    "    y.backward(t.ones(x.size()))\n",
    "    \n",
    "def f_th(x):\n",
    "    y = t.sigmoid(x)\n",
    "    y.backward(t.ones(x.size()))\n",
    "    \n",
    "x=t.randn(100, 100, requires_grad=True)\n",
    "%timeit -n 100 f_sigmoid(x)\n",
    "%timeit -n 100 f_naive(x)\n",
    "%timeit -n 100 f_th(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然f_sigmoid要比单纯利用autograd加减和乘方操作实现的函数快不少，因为f_sigmoid的backward优化了反向传播的过程。另外可以看出系统实现的built-in接口(t.sigmoid)更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小试牛刀：用Variable实现线性回归\n",
    "在上一节中讲解了利用tensor实现线性回归，在这一小节中，将讲解如何利用autograd/Variable实现线性回归，以此感受autograd的便捷之处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.manual_seed(1000)\n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    x=t.rand(batch_size,1)*5\n",
    "    y=x*2+3+t.randn(batch_size,1)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b13c68ac50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y=get_fake_data()\n",
    "plt.scatter(x.squeeze().numpy(),y.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl4leWd//H3TRKyQEKAhC0Qwq4sQiAuFUUFFLequBaXWmuHTldsp7Z1lp+/Tjudjk6HAGIt1W6j06lVqx3bUUkAKS4oCIKA5ISwhiUECEnInvOdP5IIQgJJzslZnnxe18VVcjjkfD1XffvkPvfzPM7MEBGR6Ncj3AOIiEhwKOgiIh6hoIuIeISCLiLiEQq6iIhHKOgiIh6hoIuIeISCLiLiEQq6iIhHxIbyxdLS0iwrKyuULykiEvHM4GhVHYcraqlv9NOrZwwDUhLoHd+U6PXr15eaWfq5vk9Ig56VlcW6detC+ZIiIhGrrsHPH9bvZemKQux4DXOG9+Vbs8cyfXR/nHOfPM85t7s93y+kQRcREahv9PPC+n08saKQ4rJqsjNT+cltF3D5mLRPhbyjFHQRkRCpb/Tz0gf7WLKikH3Hqpk8LJV/mTuRK8amBxTyFgq6iEgXa2j089KGYp5YUcieo1VcMLQPP7x5IleOC07IWyjoIiJdpKHRz8sb97NkhY/dR6qYlNGHZ+7PYeZ5A4Ia8hYKuohIkDU0+nmlOeS7jlQxYUgKT38+h1nnd03IWyjoIiJB0ug3/vRhMUvyCykqPcH4wSksu28aV48f2KUhb6Ggi4gEqNFvvLppP4vyfRQdPsF5g5J56t5pXDN+ID16dH3IWyjoIiKd5Pcbr24+wOJ8H4UllYwbmMzP7pnKnAmDQhryFgq6iEgH+f3GXz46wKI8H76SSsYO7M3Su6dy3cTwhLyFgi4i0k5+v/HaloMsyvOx/VAFowf0Zsm8bG6YNDisIW+hoIuInIPfb7yx9SC5eT4+PljByPReLPrcFG68YAgxERDyFucMunPul8CNQImZTWx+7HHgs0AdsAN4wMzKunJQEZFQMzPe2HqI3Dwf2w6UMzKtF7l3TeGzkyMr5C3ac4T+a+AJ4LenPLYceMTMGpxz/wY8Anwv+OOJiISemZG3rYTcvAK27C8nq38S/3HnZG6aPITYmMi96vg5g25mq51zWac99sYpX74L3B7csUREQs/MWPFxCbl5PjYXH2d4/yT+/Y7J3DIlskPeIhhr6F8Efh+E7yMiEhZmxqrth8nNK+DDfccZ1i+Rx26/gFuzM6Ii5C0CCrpz7h+ABuC5szxnPjAfIDMzM5CXExEJKjPjzYLD5Ob52Li3jKF9E/m32yZx69ShxEVRyFt0OujOuftp+rB0lplZW88zs2XAMoCcnJw2nyciEipmxmpfKbl5BWzYU0ZGaiL/euskbps6lJ6x0RfyFp0KunPuWpo+BL3CzKqCO5KISNcwM9YUlpKb52P97mMM6ZPAv8ydyB3ThkV1yFu0Z9vi74ArgTTn3D7gUZp2tcQDy5svOPOumf1tF84pItJpZsbbO46Qm1fA+7uOMbhPAj+6ZSJ35AwlPjYm3OMFTXt2ucxr5eFnumAWEZGge2fHERbmFfDezqMMSknghzdP4M4Lh3kq5C10pqiIeNLaoqaQv1t0lIEp8fzgpgncdeEwEuK8F/IWCrqIeMp7O4+Sm1fA2zuOkJ4cz6OfHc+8izI9HfIWCrqIeMK6XUdZmFfAW4VHSOsdzz/dOJ57Lu4eIW+hoItIVFu/+xi5eQX81VdKWu+e/OMN53PPxcNJ7Nl9Qt5CQReRqLRhzzEW5vlYXXCY/r168vfXn8e9lwwnqWf3zVr3/ScX8ZiXNxTz+Ovb2V9WzZDURB6eM45bsjPCPVbQfbi3jIV5Bazafph+vXry/evO475LhtMrXjnTOyDiAS9vKOaRlzZTXd8IQHFZNY+8tBnAM1HfvO84C/MKWPFxCalJcXz32nHc/5kshfwUeidEPODx17d/EvMW1fWNPP769qgP+kfFx8nNKyBvWwl9EuN4eM447r80i94K+Rn0joh4wP6y6g49Hg227D9Obp6P5VsPkZIQy99dPZYvTM8iOSEu3KNFLAVdxAOGpCZS3Eq8h6QmhmGawGw7UE5uXgGvbzlEckIs35o9lgcuyyJFIT8nBV0kyr28oZiquoYzHk+Mi+HhOePCMFHnfHywnEV5Pv73o4Mkx8eyYNYYvnjZCPokKuTtpaCLRLHTPwxtkZoYx/+/aUJUrJ8XHKpgUZ6PP28+QO/4WL45czQPXjaSPkkKeUcp6CJRrLUPQwF6xcdGfMx9hypYlN8U8qS4GL5+1Wi+dPkIUpN6hnu0qKWgi0SxaPwwtLCkgsX5hfzPpv0kxcXwlStGkZGayJOrdrB0ZaGn99B3NQVdJIpF04ehOw5Xsjjfx58+3E9iXAxfnjGK+TNGsrrgsOf30IeKgi4SxR6eM+6MNfRI+zB0Z+kJFuf7eGVjMfGxMcy/fCTzZ4ykf+94wNt76ENNQReJYi3Bi8RT/neVnmDxCh8vbyimZ2wPHrxsBF++YhRpzSFvEY3LRpFKQReJcrdkZ0REwFvsOVLF4hU+/rihmNgejgemj+DLV4xkQHJCq8+PpmWjSKegi0hQ7D1axZIVPl78oJiYHo7Pf2Y4X7liFANSWg95i2hYNooWCrqIBGTv0SqWrizkhfX76NHDcd8lw/nKlaMYeI6Qt4jkZaNoo6CLSKcUl1XzxIpCXli/F4fjnosz+cqVoxnUp30hP1WkLRtFKwVdRDpkf1k1S1cW8vy6ppB/7sJMvnrVKAb30Zp3uCnoItIuB45X8+TKHfz+/b0Yxp05w/jaVaP14WUEUdBFQiRa7yh0qLyGJ1cW8rv39uI3446cYXztqlEM7ZsU7tHkNAq6SAhE4x2FSspreHLVDv7rvT34/cbt04bytatGM6yfQh6pzhl059wvgRuBEjOb2PxYP+D3QBawC7jTzI513Zgi0S2azoYsqajhqVVFPLd2Nw1+47apGXz9qjFk9lfII117jtB/DTwB/PaUx74P5JvZT5xz32/++nvBH0/EG8J5NmR7l3oOV9Ty8zd38Oza3dQ3GnOzM/jGzNEM79+ry2eU4Dhn0M1stXMu67SHbwaubP79b4BVKOgibQrX2ZDtWeopraxl2eoifvvOLuoa/NySncE3Z44hK00hjzadXUMfaGYHAMzsgHNuQBBnEvGccJ0NebalnsvHpDWHfDe1DY3cPKXpiHxkeu8unUm6Tpd/KOqcmw/MB8jMzOzqlxOJSOE6G7KtJZ3ismouf2wl1fWN3DR5CN+cNYZRCnnU62zQDznnBjcfnQ8GStp6opktA5YB5OTkWCdfTyTqheNsyLaWegBmnT+QBbNGM3pAckhnkq7To5N/70/A/c2/vx94JTjjiEgwPTxnHAmxn/7XPMY5vnftOJbMy1bMPaY92xZ/R9MHoGnOuX3Ao8BPgOedcw8Ce4A7unJIEem449X1FJWeAHfysQHJ8fz99edH3FZJCY727HKZ18YfzQryLCJdJlrP0uyM8pp6frlmJ8+s2UlFTQPXTRzEgtljOG9QSrhHky6mM0XF86LxLM3OKK+p51drdvHMmiLKaxqYM2EgC2aNZfwQhby7UNDF86LpLM3OqKip53svbOJ/txzEDBJie/Cda8by9Zljwj2ahJiCLp7n1XtWVtY28Ju3d7F0ZSFVdSf/g1XT4Gfpyh0M7Zvkif9gSfsp6OJ5Xrtn5YnaBn7zzi5+sbqIY1X1xMeeuVnNSz+BSPt1dtuiSNR4eM44EuNiPvVYNN6zsqqugafe3MHlj63ksde2M3lYKi9/bTp1Df5Wnx/tP4FIx+kIXTwv2u9ZWV3XyH++u4ufv1nEkRN1zBibzkOzxzA1sy/gvZ9ApPMUdOkWovGeldV1jTy3djdPvbmD0so6Lh+TxkOzxzJteN9PPS9c14mRyKOgi0SYmvpGnlu7h5+t2kFpZS3TR/fnqdljycnq1+rzo/0nEAkeBV0kQtTUN/K795pCXlJRy2dG9ufJe6Zy0YjWQ36qaPwJRIJPQZduK1LOHq2pb+T37+/lyVWFHCqv5eIR/Vg8L5tLRvYP+SwS3RR06ZYi4ezR2oZGnn9/L0tX7uBgeQ0XZfVj4V1TuHRUWkheX7xHQZduKZxnj9Y1+Hl+3V6eXFnI/uM15Azvy0/vnMylo/rjnDv3NxBpg4Iu3VI4zh6ta/Dzwvp9LF1ZSHFZNVMzU/m32y/gstFpnwp5pCwFSfRR0KVbCuXe7fpGPy+u38eSFU0hnzIslR/fOokZY9LOOCKPhKUgiV4KunRLodi7Xd/o56UPmkK+71g1k4f24UdzJ3Ll2PQ2l1a8fiEx6VoKunhOe5YsunLvdkOjn5c2FPPEikL2HK3igqF9+OebJ3DVuAHnXCP36oXEJDQUdPGUjixZBHvvdkOjn5c37mfJCh+7j1QxMSOFpz+fw6zzzx3yFjqNXwKhoIunhGPJotFvvLKxmCUrCtlZeoLxg1NYdt80rh4/sMO7VnQavwRCQRdPCeWSRaPf+J8P97M430dR6QnOG5TMU/dOY86Ejoe8hU7jl0Ao6OIpoViyaPQbr25qCvmOwy0hn8o14wfRo0fg+8h1Gr90loIuntKVSxZ+v/HnzQdYnO/DV1LJ2IG9efKeqVw7ITghFwmUgi6e0hVLFn6/8b8fHWRRfgEFhyoZM6A3T9ydzfUTByvkElEUdPGcYC1Z+P3G61sOsijfx8cHKxiV3ovF87K5YdJgYhRyiUAKushpzIzXtxwiN6+Ajw9WMDK9F4s+N4UbLxiikEtECyjozrlvAV8CDNgMPGBmNcEYTCTUzIzlWw+Rm+dj64FyRqT1YuFdk7lpcoZCLlGh00F3zmUA3wTGm1m1c+554HPAr4M0m0hImBn520rIzS/go+JyhvdP4qd3TObmKUOIjdF91CV6BLrkEgskOufqgSRgf+AjiYSGmbFyewm5eT427TtOZr8kHr/9AuZmZyjkEpU6HXQzK3bO/TuwB6gG3jCzN4I2mUgXMTNWFRwmN8/Hh3vLGNYvkcduu4C5UzOIU8gligWy5NIXuBkYAZQBf3DO3Wtmz572vPnAfIDMzMwARhUJjJnxZnPIN+4tIyM1kZ/cOonbpg1VyMUTAllymQ3sNLPDAM65l4BLgU8F3cyWAcsAcnJyLIDXE+kUM+OvvlJy8wr4YE9TyH88dxK3TxtKz1iFXLwjkKDvAS5xziXRtOQyC1gXlKlEgsDMeKvwCLl5BazbfYzBfRL40S0TuTNnmCdDrjsdSSBr6Gudcy8AHwANwAaaj8RFwu3tHaXkLvfx3q6jDEpJ4Ic3T+DOC4cRHxsT7tG6hO50JBDgLhczexR4NEiziATs3aIjLFxewNqdRxmYEs8PbprAXRcOIyHOmyFvoTsdCehMUfGI93YeZeHyAt4pOsKA5Hge/ex45l2U6fmQt9CdjgQUdIkSba0Pv7/rKLl5BbxVeIT05Hj+343jufvi7hPyFrrTkYCCLlGgtfXh776wiSdXFVJwqJK03j35xxvO556Lh5PYs3uFvIXudCSgoEsUaG19uK7RT2FJJf9w/fnce0n3DXkL3elIQEGXKNDWOrDf4G9mjAzxNJFLdzoS723GFU/ZtK+szT3jGVofFvkUHaFLRNq87zi5eQXkf1xCUs8YYns4GvwnTzTW+rDImRR0iSgfFR8nN89H3rZD9EmM4zvXjOX+S7PI31ai9WGRc1DQJSJs3V9Obl4Bb2w9REpCLN++eixfmJ5FSkIcoPVhkfZQ0CWsPj5YTu5yH69tOUhyQiwPzR7DA9NH0CcxLtyjiUQdBV3CYvvBChblF/CXzQdJjo/lm7PG8OBl3g25LpwloaCgS0j5DlWQm+/jL5sP0KtnLN+YOZoHLxtBalLPcI/WZXThLAkVBV1CorCkgkX5hby6aT9JcTF89cpRfOmykfTt5d2Qt9CFsyRUFHTpUoUllSxZ4eNPH+4nMS6Gv71iFH9z+Uj6dYOQt9CFsyRUFHTpEkWHK1mc3xTy+NgY5s8YyfzLR9K/d3y4Rws5XThLQkVBl6DaWXqCJfk+Xt5YTM/YHnzp8pHMnzGStG4Y8ha6cJaEioIuQbH7yAkW5xfy8sZi4mIcX5w+gi9fMYr05O4b8ha6cJaEioIuAdlzpIolK3y8tKGY2B6OL1yaxZevGMmA5IRwjxZRdGKUhIKCLp2y92gVT6wo5MUP9tGjh+PznxnOV64YxYAUhVwkXBR0+ZRznQCz71gVS1cW8od1TSG/95LhfOXKUQxUyEXCTkGXT5ztBJgLR/RrDvleHI67L87kq1eOZlAfhVwkUijo8om2ToD5hz9upq7RD8BdFw7jq1eO1pY7kQikoMsn2jrR5URdI3dfnMnXrhqtm0qIRDDdsUg+0dZR98CUeH48d5JiLhLhFHQBoKS8hlHpvc54PDEuhkeuOz8ME4lIRwW05OKcSwWeBiYCBnzRzN4JxmASGiUVNfz8zSKefXc3DX7johH92F16gpKKWp0AIxJlAl1DXwS8Zma3O+d6AklBmEmCqK1tiIcravn5mzt4du1u6hr8zM0eyjdmjiYr7cyjdBGJDp0OunMuBZgBfAHAzOqAuuCMJcHQ2jbE77+4iVc2FvNu0VFqGxq5JTuDb8wcwwiFXCTqBXKEPhI4DPzKOTcZWA8sMLMTpz7JOTcfmA+QmZkZwMtJR7W2DbGmwc/K7YeZm53BN2aOZmR67zBNJyLBFsiHorHAVOBnZpYNnAC+f/qTzGyZmeWYWU56enoALycddbbrbS+8a4piLuIxgQR9H7DPzNY2f/0CTYGXCFBWVUev+NZ/ANP2QxFv6vSSi5kddM7tdc6NM7PtwCxga/BGk844XlXP02uK+NVbu6isbSDGORrNPvlzXYdbxLsC3eXyDeC55h0uRcADgY8knXG8up5n1uzkV2t2UlHbwPWTBrFg1li2HSjXdbhFuomAgm5mG4GcIM0inVBeU88v1+zkmTU7qahp4NoJg1gwewznD04BYNygZAVcpJvQtVyiVEVNPb96axdP/7WI8poGrhk/kAWzxzBhSJ9wjyYiYaKgR5nK2gZ+/dZOfvHXnRyvrmf2+QN5aPYYJmYo5CLdnYIeJSprG/jN27v4xV+LKKuqZ9Z5A3ho9lgmDVXIRaSJgh7hTtQ28Nt3drNs9Q6OVdUz87wBLJg1hsnDUsM9mohEGAU9QlXVNfCf7+zm56uLOHqijivHpfPQ7LFMUchFpA0KeoSprmvk2Xd38/PVOyitrGPG2HQemj2GqZl9wz2aiEQ4BT1C1NQ3hfypN4sorawlPrbpJN4dJZXsOVKloIvIOSnoYVZT38h/rd3Dz97cweGKWsYO7E15dT21DU338Dz1Rs3aTy4iZ6M7FoVJTX0jv3prJzMeW8k/v7qV0em9+f38SzhR2/jJDZlbVNc38vjr28M0qYhECx2hh1hNfSPPr9vL0pWFHCqv5eIR/Vg8L5tLRvYH2r5C4tmunCgiAgp6yNQ2NPL8+3tZunIHB8truCirHwvvmsKlo9I+9bwhqYkUtxLvtm7gLCLSQkEPotZu93b9pME8v24vT64sZP/xGnKG9+Wnd07m0lH9cc6d8T0enjPuU3cZAl0hUUTax9kpl1btajk5ObZu3bqQvV4onX67N4C4GEfv+FiOVdUzNTOVb109lstGp7Ua8tO/l66QKCItnHPrzeycF0LUEXqQtHa7t/pGazpl/4sXMWPMuUPe4pbsDAVcRDpMQQ+S1ta9oSnqV4zVrfdEpOsp6AFqaPTzxw3FxPRwNPrPXL7S7d5EJFQU9E5qaPTzysb9LFnhY9eRKob2TaSkvPZTe8j1YaaIhJKC3kGNfuNPHxazOL+QnaUnGD84hWX3TePq8QN5ZeN+fZgpImGjoLdTo994ddN+FuX7KDp8gvMGJfPUvdOYM2HgJx926sNMEQknBf0cGv3GnzcfYHG+j8KSSsYNTOZn90xlzoRB9OjRvl0rIiKhoKC3we83/vLRARbl+fCVVDJ2YG+W3j2V6yYq5CISmRT00/j9xmtbDrIoz8f2QxWMGdCbJ+7O5vqJgxVyEYloCnozv994fctBFuX7+PhgBaPSe7F4XjY3TBpMjEIuIlGg2wfdzHh9yyEW5fvYdqCckem9WPS5Kdx4wRCFXESiSsBBd87FAOuAYjO7MfCRQsPMWL71ELl5PrYeKGdEWi8W3jWZmyZnKOQiEpWCcYS+ANgGpAThe3U5MyN/Wwm5+QV8VFzO8P5J/PSOydw8ZQixMbrfh4hEr4CC7pwbCtwA/Avw7aBM1EXMjJXbS8jN87Fp33Ey+yXx+O0XMDc7QyEXEU8I9Ag9F/gukByEWbqEmbGq4DC5eT4+3FvG0L6JPHbbBcydmkGcQi4iHtLpoDvnbgRKzGy9c+7KszxvPjAfIDMzs7Mv12FmxmpfKQuXF7BxbxkZqYn85NZJ3DZtqEIuIp4UyBH6dOAm59z1QAKQ4px71szuPfVJZrYMWAZNN7gI4PXaxcxYU9gU8g/2NIX8x3Mncfu0ofSMVchFxLs6HXQzewR4BKD5CP07p8c8lMyMt3ccYeHyAtbtPsbgPgn86JaJ3JkzTCEXkW7BE/vQ395RSu5yH+/tOsqglAR+ePME7rxwGPGxMeEeTUQkZIISdDNbBawKxvfqiHeLmo7I1+48ysCUeH5w0wTuunAYCXEKuYh0P1F5hP7ezqMsXF7AO0VHSE+O59HPjmfeRZkKuYh0a1EV9HW7jrIwr4C3Co+Q1juef7pxPPdcrJCLiECUBH397mPk5hXwV18pab178o83nM89Fw8nsWfrIX95Q7HuHCQi3U5EB33DnmMszPOxuuAw/Xv15O+vP497LxlOUs+2x355QzGPvLSZ6vpGAIrLqnnkpc0AirqIeFpEBn3j3jJy8wpYtf0w/Xr15JHrzuO+z5w95C0ef337JzFvUV3fyOOvb1fQRcTTIirom/aVkZvnY8XHJfRNiuN7157H5z8znF7x7R9zf1l1hx4XEfGKiAj6R8XHyc0rIG9bCalJcTw8Zxz3X5pF7w6EvMWQ1ESKW4n3kNTEYIwqIhKxwhr0j4qPsyjfx/Kth+iTGMd3rhnL/ZdmkZwQ1+nv+fCccZ9aQwdIjIvh4TnjgjGyiEjECkvQt+4vJzevgDe2HiIlIZZvXz2WL0zPIiWAkLdoWSfXLhcR6W6cWZdfL+sTEydn22XfeZrXthwkOT6WBy8fwQPTR9AnMfCQi4h4lXNuvZnlnOt5IT1C95VUQmEp35w5mgcvG0mfpM6FXPvMRUTOFNKgD0iOZ833riI1qWenv4f2mYuItC6kQR+YknDWmLfnyFv7zEVEWhcR2xah/Ufe2mcuItK6iLnzw9mOvE/V1n5y7TMXke4uYoLe3iPvh+eMI/G0qytqn7mISAQFvb1H3rdkZ/Cvt04iIzURB2SkJvKvt07S+rmIdHsRs4bekTM8b8nOUMBFRE4TMUHXGZ4iIoGJmKCDjrxFRAIRMWvoIiISGAVdRMQjFHQREY9Q0EVEPEJBFxHxiE4H3Tk3zDm30jm3zTm3xTm3IJiDiYhIxwSybbEB+Dsz+8A5lwysd84tN7OtQZpNREQ6oNNH6GZ2wMw+aP59BbAN0CZyEZEwCcoaunMuC8gG1rbyZ/Odc+ucc+sOHz4cjJcTEZFWBBx051xv4EXgITMrP/3PzWyZmeWYWU56enqgLyciIm0IKOjOuTiaYv6cmb0UnJFERKQzAtnl4oBngG1m9h/BG0lERDojkCP06cB9wEzn3MbmX9cHaS4REemgTm9bNLM1gAviLCIiEgCdKSoi4hEKuoiIRyjoIiIeoaCLiHiEgi4i4hEKuoiIRyjoIiIeoaCLiHiEgi4i4hEKuoiIRyjoIiIeoaCLiHiEgi4i4hEKuoiIRyjoIiIeoaCLiHiEgi4i4hEKuoiIRyjoIiIeoaCLiHiEgi4i4hEKuoiIRyjoIiIeoaCLiHhEQEF3zl3rnNvunCt0zn0/WEOJiEjHdTrozrkYYClwHTAemOecGx+swUREpGMCOUK/CCg0syIzqwP+G7g5OGOJiEhHBRL0DGDvKV/va35MRETCIDaAv+taeczOeJJz84H5zV/WOuc+CuA1vSQNKA33EBFC78VJei9O0ntx0rj2PCmQoO8Dhp3y9VBg/+lPMrNlwDIA59w6M8sJ4DU9Q+/FSXovTtJ7cZLei5Occ+va87xAllzeB8Y450Y453oCnwP+FMD3ExGRAHT6CN3MGpxzXwdeB2KAX5rZlqBNJiIiHRLIkgtm9hfgLx34K8sCeT2P0Xtxkt6Lk/RenKT34qR2vRfO7IzPMUVEJArp1H8REY8ISdB1iYCTnHO/dM6VdPftm865Yc65lc65bc65Lc65BeGeKVyccwnOufeccx82vxc/CPdM4eaci3HObXDOvRruWcLJObfLObfZObexPTtdunzJpfkSAQXA1TRtdXwfmGdmW7v0hSOUc24GUAn81swmhnuecHHODQYGm9kHzrlkYD1wS3f8/4VzzgG9zKzSORcHrAEWmNm7YR4tbJxz3wZygBQzuzHc84SLc24XkGNm7dqPH4ojdF0i4BRmtho4Gu45ws3MDpjZB82/rwC20U3PNLYmlc1fxjX/6rYfbjnnhgI3AE+He5ZoE4qg6xIBclbOuSwgG1gb3knCp3mJYSNQAiw3s277XgC5wHcBf7gHiQAGvOGcW9981v1ZhSLo7bpEgHRPzrnewIvAQ2ZWHu55wsXMGs1sCk1nXF/knOuWy3HOuRuBEjNbH+5ZIsR0M5tK01Vtv9a8ZNumUAS9XZcIkO6neb34ReA5M3sp3PNEAjMrA1YB14Z5lHCZDtzUvHb838BM59yz4R0pfMxsf/P/lgB/pGkJu02hCLouESBnaP4g8Blgm5n9R7jnCSfnXLpzLrX594nAbODj8E4VHmb2iJkNNbMsmlqxwszuDfNYYeGc69W8YQDnXC/gGuCsu+O6POhm1gC0XCJgG/B8d75EgHPud8A7wDjn3D7n3IPhnilMpgP30XQEtrH51/XhHipMBgPE3NXPAAAAVklEQVQrnXObaDoAWm5m3Xq7ngAwEFjjnPsQeA/4s5m9dra/oDNFRUQ8QmeKioh4hIIuIuIRCrqIiEco6CIiHqGgi4h4hIIuIuIRCrqIiEco6CIiHvF/zSMG/lAr/mwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.071291923522949 2.965031623840332\n"
     ]
    }
   ],
   "source": [
    "w=t.rand(1,1,requires_grad=True)\n",
    "b=t.rand(1,1,requires_grad=True)\n",
    "\n",
    "losses=np.zeros(500)\n",
    "lr=0.005\n",
    "\n",
    "for ii in range(500):\n",
    "    x,y=get_fake_data(32)\n",
    "    \n",
    "    #forward\n",
    "    y_hat=x.mm(w)+b\n",
    "    loss=0.5*(y_hat-y)**2\n",
    "    loss=loss.sum()\n",
    "    losses[ii]=loss.item()\n",
    "    \n",
    "    #backward\n",
    "    loss.backward()\n",
    "    \n",
    "    #更新参数\n",
    "    w.data.sub_(lr*w.grad)\n",
    "    b.data.sub_(lr*b.grad)\n",
    "\n",
    "    #梯度清0\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    \n",
    "    \n",
    "    if ii%50 ==0:\n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0.0, 6.0).view(-1, 1)\n",
    "        y = x.mm(w.data) + b.data.expand_as(x)\n",
    "        plt.plot(x.numpy(), y.numpy()) # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=20) \n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "        \n",
    "        plt.xlim(0,5)\n",
    "        plt.ylim(0,13)   \n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "print(w.item(), b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylim(0,500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用autograd实现的线性回归最大的不同点就在于autograd不需要计算反向传播，可以自动计算微分。这点不单是在深度学习，在许多机器学习的问题中都很有用。另外需要注意的是在每次反向传播之前要记得先把梯度清零。\n",
    "\n",
    "本章主要介绍了PyTorch中两个基础底层的数据结构：Tensor和autograd中的Variable。Tensor是一个类似Numpy数组的高效多维数值运算数据结构，有着和Numpy相类似的接口，并提供简单易用的GPU加速。Variable是autograd封装了Tensor并提供自动求导技术的，具有和Tensor几乎一样的接口。autograd是PyTorch的自动微分引擎，采用动态计算图技术，能够快速高效的计算导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter2_2_autograd.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
